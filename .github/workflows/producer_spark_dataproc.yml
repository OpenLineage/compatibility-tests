name: Spark Dataproc

on:
  workflow_call:
    secrets:
      gcpKey:
        required: true
    inputs:
      release:
        description: "release tag of OpenLineage to use"
        type: string
      get-circleci-artifacts:
        description: "Should the artifact be downloaded from maven repo or circleci"
        type: boolean

jobs:
  run-spark-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: GCP authorization
        id: gcp-auth
        uses: 'google-github-actions/auth@v2'
        with:
          credentials_json: '${{ secrets.gcpKey }}'

      - name: Get OL artifacts
        if: ${{ !inputs.get-circleci-artifacts }}
        id: get_ol_artifacts
        uses: ./.github/actions/get_openlineage_artifacts
        with:
          version: ${{ inputs.release }}
          skip-flink: 'true'
          skip-s3: 'true'
          skip-gcp-lineage: 'true'
          skip-sql: 'true'
          credentials: ${{ steps.gcp-auth.outputs.credentials_file_path }}

      - name: Get unreleased OL artifacts
        if: ${{ inputs.get-circleci-artifacts }}
        id: get-unreleased
        uses: ./.github/actions/get_unreleased_openlineage_artifacts
        with:
          version: ${{ inputs.release }}

      - name: Upload openlineage spark integration to GCS
        id: upload-spark-integration
        if: ${{ !inputs.get-circleci-artifacts }}
        uses: ./.github/actions/upload_artifacts
        with:
          local-file-path: ${{ steps.get_ol_artifacts.outputs.spark }}
          gcs-path: "gs://open-lineage-e2e/jars"
          credentials: ${{ steps.gcp-auth.outputs.credentials_file_path }}

      - name: Upload openlineage GCS transport to GCS
        id: upload-gcs-transport
        if: ${{ !inputs.get-circleci-artifacts }}
        uses: ./.github/actions/upload_artifacts
        with:
          local-file-path: ${{ steps.get_ol_artifacts.outputs.gcs-transport }}
          gcs-path: "gs://open-lineage-e2e/jars"
          credentials: ${{ steps.gcp-auth.outputs.credentials_file_path }}

      - name: Upload initialization actions to GCS
        id: upload-initialization-actions
        uses: ./.github/actions/upload_artifacts
        with:
          local-file-path: producer/spark_dataproc/runner/get_openlineage_jar.sh
          gcs-path: "gs://open-lineage-e2e/scripts"
          credentials: ${{ steps.gcp-auth.outputs.credentials_file_path }}

      - name: Set up Python 3.11
        uses: actions/setup-python@v3
        with:
          cache: "pip"
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pytest
          if [ -f producer/spark_dataproc/runner/requirements.txt ]; then pip install -r producer/spark_dataproc/runner/requirements.txt; fi


        #TODO - we should be able to set the release of OL spark integration, not have it hardcoded. Not sure about the bigquery jar but probably something similar
      - name: Start producer
        run: |
          python producer/spark_dataproc/runner/dataproc_workflow.py create-cluster \
          --project-id gcp-open-lineage-testing \
          --region us-west1 \
          --cluster-name dataproc-producer-test-${{ github.run_id }} \
          --credentials-file ${{ steps.gcp-auth.outputs.credentials_file_path }} \
          --metadata "SPARK_BQ_CONNECTOR_URL=gs://open-lineage-e2e/jars/spark-3.5-bigquery-0.41.0.jar,OPENLINEAGE_SPARK_URL=${{ steps.upload-spark-integration.outputs.uploaded-file }}" \
          --initialization-actions="${{ steps.upload-initialization-actions.outputs.uploaded-file }}"

      - name: Set producer output event dir
        id: set-producer-output
        run: | 
          echo "event_dir=/tmp/producer-$(date +%s%3N)" >> $GITHUB_OUTPUT

      - name: Run producer jobs and create OL events
        id: run-producer
        run: |
          for scenario_path in producer/spark_dataproc/scenarios/*
          do
              scenario="${scenario_path##*/}"
              run_script=$(find "$scenario_path/test/" -maxdepth 1 -type f -name "*.py" | head -n 1)
              
              echo "Running spark job for scenario: $scenario"

              python producer/spark_dataproc/runner/dataproc_workflow.py run-job \
              --project-id gcp-open-lineage-testing \
              --region us-west1 \
              --cluster-name "dataproc-producer-test-${{ github.run_id }}" \
              --gcs-bucket open-lineage-e2e \
              --python-job "$run_script" \
              --jars "${{ steps.upload-gcs-transport.outputs.uploaded-file }}" \
              --spark-properties "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener,spark.sql.warehouse.dir=/tmp/warehouse,spark.openlineage.transport.type=gcs" \
              --output-directory "${{ steps.set-producer-output.outputs.event_dir }}/$scenario" \
              --credentials-file "${{ steps.gcp-auth.outputs.credentials_file_path }}" \
              --dataproc-image-version 2.2-ubuntu22 \
              || { echo "Error: Spark job failed for scenario: $scenario"; }

              echo "Finished running spark job for scenario: $scenario"
          done
      
      - name: Terminate producer cluster
        run: |
          python producer/spark_dataproc/runner/dataproc_workflow.py terminate-cluster \
          --project-id gcp-open-lineage-testing \
          --region us-west1 \
          --cluster-name dataproc-producer-test-${{ github.run_id }} \
          --credentials-file ${{ steps.gcp-auth.outputs.credentials_file_path }}

      - name: Validation
        uses: ./.github/actions/run_event_validation
        with:
          component: 'spark_dataproc'
          version: ${{ inputs.release }}
          event-directory: ${{ steps.set-producer-output.outputs.event_dir }}
          target-path: 'spark-dataproc-report.json'

      - uses: actions/upload-artifact@v4
        with:
          name: spark-dataproc-report
          path: spark-dataproc-report.json
          retention-days: 1
