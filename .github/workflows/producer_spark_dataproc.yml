name: Spark Dataproc

on:
  workflow_call:
    secrets:
      gcpKey:
        required: true
    inputs:
      release:
        description: "release tag of OpenLineage to use"
        type: string
      get-latest-snapshots:
        description: "Should the artifact be downloaded from maven repo or circleci"
        type: string

jobs:
  run-spark-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: GCP authorization
        id: gcp-auth
        uses: 'google-github-actions/auth@v2'
        with:
          credentials_json: '${{ secrets.gcpKey }}'

      - name: Get OL artifacts
        id: get-ol-artifacts
        uses: ./.github/actions/get_openlineage_artifacts
        with:
          version: ${{ inputs.release }}
          get-latest-snapshots: ${{ inputs.get-latest-snapshots }}
          skip-flink: 'true'
          skip-s3: 'true'
          skip-gcp-lineage: 'true'
          skip-sql: 'true'

      # This should be somehow automated and return the newest version
#      - name: Download Big Query Spark Maven Artifact
#        id: download-spark-bq-connector
#        uses: clausnz/github-action-download-maven-artifact@master
#        with:
#          url: 'https://repo1.maven.org'
#          repository: 'maven2'
#          groupId: 'com.google.cloud.spark'
#          artifactId: 'spark-3.5-bigquery'
#          version: '0.42.0'
#          extension: 'jar'

      - name: Upload openlineage spark integration to GCS
        id: upload-spark-integration
        uses: ./.github/actions/upload_artifacts
        with:
          local-file-path: ${{ steps.get-ol-artifacts.outputs.spark }}
          gcs-path: "gs://open-lineage-e2e/jars"
          credentials: ${{ steps.gcp-auth.outputs.credentials_file_path }}

      - name: Upload openlineage GCS transport to GCS
        id: upload-gcs-transport
        uses: ./.github/actions/upload_artifacts
        with:
          local-file-path: ${{ steps.get-ol-artifacts.outputs.gcs }}
          gcs-path: "gs://open-lineage-e2e/jars"
          credentials: ${{ steps.gcp-auth.outputs.credentials_file_path }}

#      - name: Upload Spark BigQuery connector to GCS
#        id: upload-spark-bq-connector
#        uses: ./.github/actions/upload_artifacts
#        with:
#          local-file-path: ${{ steps.download-spark-bq-connector.outputs.file }}
#          gcs-path: "gs://open-lineage-e2e/jars"
#          credentials: ${{ steps.gcp-auth.outputs.credentials_file_path }}

      - name: Upload initialization actions to GCS
        id: upload-initialization-actions
        uses: ./.github/actions/upload_artifacts
        with:
          local-file-path: producer/spark_dataproc/runner/get_openlineage_jar.sh
          gcs-path: "gs://open-lineage-e2e/scripts"
          credentials: ${{ steps.gcp-auth.outputs.credentials_file_path }}

      - name: Set up Python 3.11
        uses: actions/setup-python@v3
        with:
          cache: "pip"
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pytest
          if [ -f producer/spark_dataproc/runner/requirements.txt ]; then pip install -r producer/spark_dataproc/runner/requirements.txt; fi

      - name: Start producer
        run: |
          python producer/spark_dataproc/runner/dataproc_workflow.py create-cluster \
          --project-id gcp-open-lineage-testing \
          --region us-west1 \
          --cluster-name dataproc-producer-test-${{ github.run_id }} \
          --credentials-file ${{ steps.gcp-auth.outputs.credentials_file_path }} \
          --metadata "SPARK_BQ_CONNECTOR_URL=gs://open-lineage-e2e/jars/spark-3.5-bigquery-0.0.1-SNAPSHOT.jar,OPENLINEAGE_SPARK_URL=${{ steps.upload-spark-integration.outputs.uploaded-file }},SPARK_SPANNER_CONNECTOR_URL=gs://open-lineage-e2e/jars/spark-3.1-spanner-1.1.0.jar" \
          --initialization-actions="${{ steps.upload-initialization-actions.outputs.uploaded-file }}"
#          --metadata "SPARK_BQ_CONNECTOR_URL=${{ steps.upload-spark-bq-connector.outputs.uploaded-file }},OPENLINEAGE_SPARK_URL=${{ steps.upload-spark-integration.outputs.uploaded-file }}" \

      - name: Set producer output event dir
        id: set-producer-output
        run: | 
          echo "event_dir=/tmp/producer-$(date +%s%3N)" >> $GITHUB_OUTPUT

      - name: Run producer jobs and create OL events
        id: run-producer
        run: |
          for scenario_path in producer/spark_dataproc/scenarios/*
          do
              scenario="${scenario_path##*/}"
              run_script=$(find "$scenario_path/test/" -maxdepth 1 -type f -name "*.py" | head -n 1)
              
              echo "Running spark job for scenario: $scenario"

              python producer/spark_dataproc/runner/dataproc_workflow.py run-job \
              --project-id gcp-open-lineage-testing \
              --region us-west1 \
              --cluster-name "dataproc-producer-test-${{ github.run_id }}" \
              --gcs-bucket open-lineage-e2e \
              --python-job "$run_script" \
              --jars "${{ steps.upload-gcs-transport.outputs.uploaded-file }}" \
              --spark-properties "spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener,spark.sql.warehouse.dir=/tmp/warehouse,spark.openlineage.transport.type=gcs" \
              --output-directory "${{ steps.set-producer-output.outputs.event_dir }}/$scenario" \
              --credentials-file "${{ steps.gcp-auth.outputs.credentials_file_path }}" \
              --dataproc-image-version 2.2-ubuntu22 \
              || { echo "Error: Spark job failed for scenario: $scenario"; }

              echo "Finished running spark job for scenario: $scenario"
          done
      
      - name: Terminate producer cluster
        run: |
          python producer/spark_dataproc/runner/dataproc_workflow.py terminate-cluster \
          --project-id gcp-open-lineage-testing \
          --region us-west1 \
          --cluster-name dataproc-producer-test-${{ github.run_id }} \
          --credentials-file ${{ steps.gcp-auth.outputs.credentials_file_path }}

      - name: Validation
        uses: ./.github/actions/run_event_validation
        with:
          component: 'spark_dataproc'
          tag: ${{ inputs.get-latest-snapshots == 'true' && 'main' || inputs.release }}
          release: ${{ inputs.release }}
          event-directory: ${{ steps.set-producer-output.outputs.event_dir }}
          target-path: 'spark-dataproc-report.json'

      - uses: actions/upload-artifact@v4
        with:
          name: spark-dataproc-report
          path: spark-dataproc-report.json
          retention-days: 1
